{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spacy is open source library for NLP.\n",
    "NLP is a subfield of artificial intelligence concerned with the interaction of human and computer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "use cases are ,\n",
    "automatic summarization\n",
    "named entity recognition\n",
    "qa systems\n",
    "sentiment analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "english language is the default model for spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the language model instance using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_text=('this is the tutorial for'' spacy in natural language prodd')#we are not using commas\n",
    "introduction_doc=nlp(introduction_text)#creation a doc element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'tutorial', 'for', 'spacy', 'in', 'natural', 'language', 'prodd']\n"
     ]
    }
   ],
   "source": [
    "#printing tokens\n",
    "print([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to read a text file use spacy open command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  sentence detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus proto is a python developer currebtly orking for a london fintech company.\n",
      "He is interested in learning natural language proceessing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#locating the start and end of sentence in a given text \n",
    "#we use a sents property\n",
    "about_text=('gus proto is a python developer currebtly'\n",
    "           ' orking for a london fintech'\n",
    "           ' company. He is interested in learning'\n",
    "           ' natural language proceessing.')\n",
    "about_doc=nlp(about_text)\n",
    "sentences=list(about_doc.sents)#converting it to list for for loop\n",
    "for sentence in sentences:#printing the sentences\n",
    "    print(sentence)\n",
    "len(sentences)#number of sentences\n",
    "#full stop as the sentence delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus, can u ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "#elipsis... used as limiter\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text ==  '...':\n",
    "            doc[tocken.i+1].is_sent_start=True#making next text fter elipsis as a start of senence\n",
    "        return doc\n",
    "elipsis_text=('gus, can u ... never mind, I forgot'' what I was saying. So, do you think'' we should ...' )\n",
    "custom_nlp=spacy.load('en_core_web_sm')\n",
    "custom_nlp.add_pipe(set_custom_boundaries,before='parser')\n",
    "custom_elipsis_doc=custom_nlp(elipsis_text)\n",
    "custom_elipsis_sentences=list(custom_elipsis_doc.sents)\n",
    "for sentence in custom_elipsis_sentences:\n",
    "    print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus, can u ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence setection without  customization\n",
    "elipsis_doc=nlp(elipsis_text)\n",
    "elipsis_sentence=list(elipsis_doc.sents)\n",
    "for sentence in elipsis_sentence:\n",
    "    print(sentence)\n",
    "len(elipsis_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus 0\n",
      "proto 4\n",
      "is 10\n",
      "a 13\n",
      "python 15\n",
      "developer 22\n",
      "currebtly 32\n",
      "orking 42\n",
      "for 49\n",
      "a 53\n",
      "london 55\n",
      "fintech 62\n",
      "company 70\n",
      ". 77\n",
      "He 79\n",
      "is 82\n",
      "interested 85\n",
      "in 96\n",
      "learning 99\n",
      "natural 108\n",
      "language 116\n",
      "proceessing 125\n",
      ". 136\n"
     ]
    }
   ],
   "source": [
    "#it is for identifying tokens in the text\n",
    "#token can be printed uhxinng iterative approch\n",
    "for token in about_doc:\n",
    "    print(token,token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus 0 gus  True False False xxx False\n",
      "proto 4 proto  True False False xxxx False\n",
      "is 10 is  True False False xx True\n",
      "a 13 a  True False False x True\n",
      "python 15 python  True False False xxxx False\n",
      "developer 22 developer  True False False xxxx False\n",
      "currebtly 32 currebtly  True False False xxxx False\n",
      "orking 42 orking  True False False xxxx False\n",
      "for 49 for  True False False xxx True\n",
      "a 53 a  True False False x True\n",
      "london 55 london  True False False xxxx False\n",
      "fintech 62 fintech  True False False xxxx False\n",
      "company 70 company True False False xxxx False\n",
      ". 77 .  False True False . False\n",
      "He 79 He  True False False Xx True\n",
      "is 82 is  True False False xx True\n",
      "interested 85 interested  True False False xxxx False\n",
      "in 96 in  True False False xx True\n",
      "learning 99 learning  True False False xxxx False\n",
      "natural 108 natural  True False False xxxx False\n",
      "language 116 language  True False False xxxx False\n",
      "proceessing 125 proceessing True False False xxxx False\n",
      ". 136 . False True False . False\n"
     ]
    }
   ],
   "source": [
    "#token attributes\n",
    "for token in about_doc:\n",
    "    print(token,token.idx,token.text_with_ws,token.is_alpha,token.is_punct,token.is_space,token.shape_,token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop words detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of stop words in spacy# list\n",
    "stpow=spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(stpow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "against\n"
     ]
    }
   ],
   "source": [
    "#for printing the stop words\n",
    "for stopwprds in list(stpow)[:1]:\n",
    "    print(stopwprds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus\n",
      "proto\n",
      "python\n",
      "developer\n",
      "currebtly\n",
      "orking\n",
      "london\n",
      "fintech\n",
      "company\n",
      ".\n",
      "interested\n",
      "learning\n",
      "natural\n",
      "language\n",
      "proceessing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#printing the tokend=s in the doc object\n",
    "for token in about_doc:\n",
    "    if token.is_stop==False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus\n",
      "proto\n",
      "python\n",
      "developer\n",
      "currebtly\n",
      "orking\n",
      "london\n",
      "fintech\n",
      "company\n",
      ".\n",
      "interested\n",
      "learning\n",
      "natural\n",
      "language\n",
      "proceessing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gus, proto, python, developer, currebtly, orking, london, fintech, company, ., interested, learning, natural, language, proceessing, .]\n"
     ]
    }
   ],
   "source": [
    "#printing a list of token words without stop word in doc\n",
    "about_no_stop_doc=[token for token in about_doc if not token.is_stop]\n",
    "print(about_no_stop_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus gus\n",
      "is be\n",
      "helping help\n",
      "organize organize\n",
      "a a\n",
      "   \n",
      "developer developer\n",
      "conference conference\n",
      "on on\n",
      "application application\n",
      "of of\n",
      "natural natural\n",
      "language language\n",
      "processing processing\n",
      ". .\n",
      "he -PRON-\n",
      "keeps keep\n",
      "organizing organize\n",
      "the the\n",
      "local local\n",
      "meetups meetup\n",
      "and and\n",
      "several several\n",
      "internal internal\n",
      "talks talk\n",
      "at at\n",
      "his -PRON-\n",
      "workplace workplace\n"
     ]
    }
   ],
   "source": [
    "#reducing the inflated forms of the word\n",
    "#the root word is called lemma\n",
    "conference_help_text=('gus is helping organize a  developer'' conference on application of natural language'\n",
    "                      ' processing. he keeps organizing the local meetups'' and several internal talks at his workplace')\n",
    "conference_help_doc=nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for printing the common_words and unique_words performing statistical analysis\n",
    "complete_text=('gus proto is a python developer currently'' worrking for a london-based fintech company. he is''interested in leraning natural language processing.'\n",
    "              'there is a developer conference happening on 21 july''2019 in london. it is titled' \n",
    "               'application of natural' 'language precessing there is a helpline number''available at 1-1234567891. gus is helping organize it'\n",
    "              'he keeps organizing local ptythn metups and several''internal talks at his workplace. gudd is also presenting''atalk. the talk will introduce the resderr about the use '\n",
    "            )\n",
    "complete_doc=nlp(complete_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gus', 2), ('developer', 2), ('london', 2), ('proto', 1), ('python', 1)]\n"
     ]
    }
   ],
   "source": [
    "words=[token.text for token in complete_doc if not token.is_stop and not token.is_punct]#removing punct and stop words\n",
    "from collections import Counter\n",
    "word_freq=Counter(words)\n",
    "common_words=word_freq.most_common(5)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proto', 'python', 'currently', 'worrking', 'based', 'fintech', 'company', 'isinterested', 'leraning', 'natural', 'language', 'processing.there', 'conference', 'happening', '21', 'july2019', 'titledapplication', 'naturallanguage', 'precessing', 'helpline', 'numberavailable', '1', '1234567891', 'helping', 'organize', 'ithe', 'keeps', 'organizing', 'local', 'ptythn', 'metups', 'severalinternal', 'talks', 'workplace', 'gudd', 'presentingatalk', 'talk', 'introduce', 'resderr', 'use']\n"
     ]
    }
   ],
   "source": [
    "unique_words=[word for(word,freq) in word_freq.items() if freq == 1]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[fintech, interested, natural]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#asigning pos tag to each token\n",
    "#using pos tags u can acess the particular set of words\n",
    "#printing thre verb and the adjective\n",
    "nouns=[]\n",
    "adj=[]\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ =='ADJ':\n",
    "        adj.append(token)\n",
    "nouns,\n",
    "adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualisation using displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"69f7d552d5a2499687516c88e022a96c-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-69f7d552d5a2499687516c88e022a96c-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-69f7d552d5a2499687516c88e022a96c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displacy is the visualisation tool for spacy\n",
    "#it shows the dependency of parts of speech tagging of each token\n",
    "from spacy import displacy\n",
    "interest_text=('He is interested in learning'' natural language processing.')\n",
    "interest_text_doc=nlp(interest_text)\n",
    "displacy.render(interest_text_doc,style='dep',jupyter=True)#render function used to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([language, processing],)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns=[]\n",
    "adj=[]\n",
    "for token in interest_text_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ =='ADJ':\n",
    "        adj.append(token)\n",
    "nouns,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gus',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'developer',\n",
       " 'currently',\n",
       " 'worrke',\n",
       " 'london',\n",
       " 'base',\n",
       " 'fintech',\n",
       " 'company',\n",
       " 'isintereste',\n",
       " 'lerane',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.there',\n",
       " 'developer',\n",
       " 'conference',\n",
       " 'happen',\n",
       " '21',\n",
       " 'july2019',\n",
       " 'london',\n",
       " 'titledapplication',\n",
       " 'naturallanguage',\n",
       " 'precess',\n",
       " 'helpline',\n",
       " 'numberavailable',\n",
       " '1',\n",
       " '1234567891',\n",
       " 'gus',\n",
       " 'help',\n",
       " 'organize',\n",
       " 'ithe',\n",
       " 'keep',\n",
       " 'organize',\n",
       " 'local',\n",
       " 'ptythn',\n",
       " 'metup',\n",
       " 'severalinternal',\n",
       " 'talk',\n",
       " 'workplace',\n",
       " 'gudd',\n",
       " 'presentingatalk',\n",
       " 'talk',\n",
       " 'introduce',\n",
       " 'resderr',\n",
       " 'use']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the text in an analyzable format\n",
    "def is_token_allowed(token):\n",
    "    if(not token or  not token.string.strip() or  token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "def preprocess_token(token):#it converts to lower case and lemmatize the text\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "filtered_tokens=[preprocess_token(token) \n",
    "                 for token in complete_doc if is_token_allowed(token) ]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gus NNP learning nsubj\n",
      "is VBZ learning aux\n",
      "learning VBG learning ROOT\n",
      "piano NN learning dobj\n"
     ]
    }
   ],
   "source": [
    "#extracting the gramatial structure of a sentence\n",
    "#dependency relationship between head words and depemdents\n",
    "#the verb is usually the head of the sentence\n",
    "#words are the nodes and dependencies are the relationship\n",
    "piano_text='gus is learning piano'\n",
    "piano_doc=nlp(piano_text)\n",
    "for token in piano_doc:\n",
    "    \n",
    "    print(token.text,token.tag_,token.head.text,token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## navigating the tree and sub tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'python', 'working']\n"
     ]
    }
   ],
   "source": [
    "# spacy provides attributes like children,lefts,rights and sub tree to navigate parse tree\n",
    "one_line_about_text=('gus proto is a python developer'' currently working for london based fintech company')\n",
    "one_line_about_doc=nlp(one_line_about_text)\n",
    "#extracting children of developer\n",
    "print([token.text for token in one_line_about_doc[5].children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "#extracting previous neighbor node of developer\n",
    "print(one_line_about_doc[5].nbor(-1))# for single elements we dont use for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently\n"
     ]
    }
   ],
   "source": [
    "#extracting next node of a developer\n",
    "print(one_line_about_doc[5].nbor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'python']\n"
     ]
    }
   ],
   "source": [
    "#extracting all the tokens on the lefft of developer\n",
    "print([token.text for token in one_line_about_doc[5].lefts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in one_line_about_doc[5].rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a, python, developer, currently, working, for, london, based, fintech, company]\n"
     ]
    }
   ],
   "source": [
    "# printing subtree of a developer\n",
    "print(list(one_line_about_doc[5].subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a python developer currently working for london based fintech company\n"
     ]
    }
   ],
   "source": [
    "# building a fuction that takes subtree as argument returns string of appended words\n",
    "def flatten_tree(tree):\n",
    "    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
    "print(flatten_tree(one_line_about_doc[5].subtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting phrases from unstructures text\n",
    "## chunking groups the adjacemt tokens into phrases  on the basis of pos tags\n",
    "## noun phrases,verb phrases,prepositional phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noun phrase detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps in infer that what os being talked in a sentence\n",
    "# we use noun_chunks for noun_phrase detectioin\n",
    "conference_text=('There is a developer conference'' happening on 21 july 2019 in london.')\n",
    "conference_doc=nlp(conference_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a developer conference\n",
      "21 july\n",
      "london\n"
     ]
    }
   ],
   "source": [
    "#extracting noun_phrases\n",
    "for chunks in conference_doc.noun_chunks:\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verb phrase detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-5c06ca0a2f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#helps in knowing the actions involved with noun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtextacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'textacy' is not defined"
     ]
    }
   ],
   "source": [
    "#helps in knowing the actions involved with noun\n",
    "textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the named entity and classifying them to predefined categories\n",
    "# we use ents in doc objects\n",
    "piano_class_text=('Great piano academy is situated'' in mayfair or the city of london and has' \n",
    "                  ' world-class piano instructors.')\n",
    "piano_class_doc=nlp(piano_class_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london 58 64 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "for ent in piano_class_doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_,spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Great piano academy is situated in mayfair or the city of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    london\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_class_doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
